<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="STEP-Mover: Stratified and Tiered Elimination Process for Efficient LiDAR Dynamic Removal">
  <meta name="keywords" content="STEP-Mover: Stratified and Tiered Elimination Process for Efficient LiDAR Dynamic Removal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>STEP-Mover</title>

  <!-- Bootstrap -->
  <link rel="stylesheet" href="./web/static/css/bootstrap-4.4.1.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  
  
  <link rel="stylesheet" href="./web/static/css/bulma.min.css">
  <link rel="stylesheet" href="./web/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./web/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./web/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./web/static/css/index.css">
  <link rel="icon" href="./web/static/images/logo.webp">

  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./web/static/js/fontawesome.all.min.js"></script>
  <script src="./web/static/js/bulma-carousel.min.js"></script>
  <script src="./web/static/js/bulma-slider.min.js"></script>
  <script src="./web/static/js/index.js"></script>
  <script src="./web/static/js/app.js"></script>
  <script src="./web/static/js/video_comparison.js"></script>

  <link rel="stylesheet" href="./web/static/css/dics.original.css">
  <script src="./web/static/js/event_handler.js"></script>
  <script src="./web/static/js/dics.original.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="margin-bottom: 0"><img src="web/static/images/logo.png" width="80">   <strong>STEP-Mover</strong></h1>
          <br>
          <h2 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">STEP-Mover: Stratified and Tiered Elimination Process for Efficient LiDAR Dynamic Removal</h2>
          <!-- <br> -->
          <div class="ral2025" style="margin-top: 10px; margin-bottom: 20px;">
            <h2 class="title is-4">RA-L 2025</h2>
          </div>

          

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yaepiii.github.io/">Yanpeng Jia</a><sup>1,2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://people.ucas.ac.cn/~siawangting1">Ting Wang</a><sup>1*</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://people.ucas.edu.cn/~shaoshiliang">Shiliang Shao</a><sup>1</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://chen-xieyuanli.github.io/">Xieyuanli Chen</a><sup>3</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>
          <div class="column is-full_width">
            <h2 class="is-size-6">* Corresponding Author</h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>State Key Laboratory of Robotics at Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences, Beijing, China</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>National University of Defense Technology, Beijing, China</span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2408.05981"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            
              <span class="link-block">
                <a href="https://yaepiii.github.io/STEP-Mover/"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Website</span>
                  </a>
              </span>  
          </div>







          
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="width: 70%; height: 70%; margin: 0 auto; display: flex; justify-content: center; align-items: center;">
        <video id="teaser" autoplay muted loop style="width: 100%; height: 100%;">
          <source src="web/resources/teaser_nerf-on-the-go.mp4" type="video/mp4">
        </video>
      </div> -->
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <!-- <br><br><br>
      <h2 class="subtitle has-text-centered">
        <strong style="font-size: 0.9em;">NeRF <em>On-the-go</em></strong> enables novel view synthesis in in-the-wild scenes from casually captured images.
    </h2>
    </div>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
      
          <video class="video" width="80%" id="xyalias6" loop playsinline autoplay muted src="web/resources/yard_high_pj.mp4" onplay="resizeAndPlay(this)" style="height: 0px;"></video>
          <canvas height=0 class="videoMerge" id="xyalias6Merge"></canvas>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reconstructions</h2>

        <div class="embed-responsive embed-responsive-16by9">

          <iframe style="clip-path: inset(1px 1px)" src="https://sketchfab.com/playlists/embed?collection=abee3cc1a7a7436c804f2bd3aadc2acd" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true" width="100%" height="100%" frameborder="0"></iframe>
        </div>
        

      </div>
    </div>

  </div>
</section>

-->

<section class="hero is-light">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3" style="margin-top: -30px">Video</h2>
      <video controls autoplay muted loop style="max-width: 100%; height: auto;">
        <source src="https://raw.githubusercontent.com/Yaepiii/STEP-Mover/main/web/resources/video.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Clean and high-fidelity global static maps are essential for enabling precise localization and efficient navigation.
            However, dynamic objects in the environment may introduce
            "ghost trail" during map construction, which significantly degrades map quality and limits the execution of downstream tasks.
            Existing dynamic removal methods often struggle to balance
            computational efficiency and accuracy. To address this issue, this
            paper presents a Stratified and Tiered dynamic object removal
            framework, STEP-Mover, which efficiently filters dynamic points
            from the global map to preserve its high-fidelity feature. The
            proposed method generates a voxel-based multi-scale descriptor
            and progressively localizes dynamic regions in a Coarse-to-Fine-to-Refine method by leveraging feature differences between the
            query scan and the prior map. To compensate for blind spots
            in the query scan, a high point retrieval strategy is introduced
            to retrieve static points outside the field of view. Finally, the
            roughly extracted ground points are used as priors, and union-find-based connectivity clustering is applied to refine ground
            point recovery. Extensive comparative experiments are conducted
            on the SemanticKITTI, HeLiMOS, and self-collected M2UD
            datasets. Quantitative and qualitative results demonstrate that,
            compared with other baselines, the proposed method achieves
            superior performance in both accuracy and efficiency.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <!-- <div class="container is-max-desktop">
      <div class="hero-body">
        <div style="width: 70%; height: 70%; margin: 0 auto; display: flex; justify-content: center; align-items: center;">
          <video id="teaser" autoplay muted loop style="width: 100%; height: 100%;">
            <source src="web/resources/CAD-Mesher.mp4" type="video/mp4">
          </video>
        </div>-->
        <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
        <!-- <br><br><br>
        <h2 class="subtitle has-text-centered">
          <strong style="font-size: 0.9em;">CAD-Mesher</strong> can easily integrating with various LiDAR odometry to further improve localization accuracy, 
          filtering dynamic objects and generating a accurate, consecutive, and dense mesh map.
        </h2>
      </div>
    </div> -->

    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/PErRFQMZn4I?si=1gtgqwmvPs3yNKDP"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">System Framework</h2>

        <div style="width: 100%; margin: 0 auto; display: flex; justify-content: center;">
          <img src="./web/resources/figure2.png" style="width: 150%;">
        </div>
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            Figure illustrates the overall system architecture. The input to
            the system includes the prior map M, which contains dynamic
            points to be removed, the query scans in the LiDAR coordinate
            system L, and the pose transformation from the LiDAR coordinate system to the world coordinate system W. The
            system first applies voxel hashing to partition and manage
            both the prior map and the query scans, enabling efficient data
            processing. Multi-scale descriptors are then generated to facilitate subsequent dynamic point identification. Subsequently, a
            coarse-to-fine-to-refine dynamic point elimination strategy is
            employed, in which dynamic voxels are detected based on the
            feature differences between the query scans descriptor and
            the prior map descriptor. To mitigate quantization effects,
            dynamic voxels are further refined using union-find-based
            connectivity clustering, followed by fine-level point retrieval.
            Static points outside the sensor's FOV are restored through a
            high point extension method and fine ground fitting guided by
            the coarsely estimated prior ground. Ultimately, this pipeline
            enables efficient elimination of dynamic points from the prior
            map and outputs a clean, high-fidelity global point cloud map.
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px"><i>On-the-go</i> Dataset</h2>
        <video class="video" controls muted autoplay loop src="web/resources/on-the-go.mp4"></video>
        <div class="content has-text-justified">
          <p>
            To rigorously evaluate our approach in real-world settings, we captured a dataset that contains 12 casually captured sequences, including 10 outdoor and 2 indoor scenes. 
            We name this dataset On-the-go dataset. This dataset features a wide range of dynamic objects including pedestrians, cyclists, strollers, toys, cars, robots, and trams, along with diverse occlusion ratios ranging from 5% to 30%.

          </p>
        </div>
        
     
    </div>

  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Experiment Setup</h2>

        <div class="content has-text-justified">
          <p>
            To comprehensively evaluate the performance of the baseline methods, we conduct extensive experiments on the SemanticKITTI, HeLiMOS, and M2UD datasets.
            These datasets cover diverse environments including urban
            streets, highways, and rural areas, and involve various LiDAR
            types and configurations. Detailed dataset statistics are provided in Table. I.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table1.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <br>
        <br><br>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Quantitative Results</h2>

        <div class="content has-text-justified">
          <p>
            1) SemanticKITTI Dataset: The comparison results on the
            SemanticKITTI dataset are presented in Table. II. Learning-based methods achieve competitive performance on most sequences, benefiting from extensive pre-training on the dataset.
            Among traditional methods, ERASOR suffers from quantization errors, which lead to the removal of many static
            points and thus degrade its overall performance. OctoMap and
            OctoMapFG remove dynamic objects using ray-casting-based
            visibility checking. However, inaccuracies in incident angle
            and pose estimation reduce their static accuracy. DUFOMap
            and BeautyMap performs well across multiple sequences by
            incorporating localization uncertainty or combining binary
            encoding and static recovery, respectively. However, their
            performance drops on Sequence 02, which significantly differs
            from other sequences and likely reflects the challenges posed
            by rural environmental characteristics. In contrast, our method
            maintains stable performance across all five sequences. Although our method does not always achieve the highest scores
            in either SA or DA, it consistently obtains the highest HA
            results with the most times, demonstrating its superior overall
            performance.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table2.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>


        <div class="content has-text-justified">
          <p>
            HeLiMOS Dataset: To further evaluate the generalization ability of the baselines, we conduct experiments on the
            HeLiMOS dataset involving multiple types of LiDAR
            sensors. Ground-truth poses are used as input to evaluate the
            effect of localization errors-free, and the results are presented
            in Table.III. Learning-based methods exhibit significant domain adaptation issues, resulting in substantial performance
            degradation or even complete failure when applied to different
            LiDAR types. For sparse LiDAR data, Removert fails to
            generate robust range image projections and performs poorly
            on the Velodyne sequence. ERASOR retains ability for dynamic object removal across varying LiDAR types though
            the scan ratio test. However, its reliance on polar coordinate
            partitioning can cause large blank bins, leading to inefficient
            resource utilization. BeautyMap fails on solid-state LiDAR,
            likely due to FOV inconsistencies negatively affecting the
            static recovery module. Our method maintains stable performance across all sequences and achieves competitive results on
            multiple evaluation metrics, which is attributed to the voxel-based representation and the use of multi-scale descriptors,
            enhancing the generalization capability of our approach.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table3.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>



        <div class="content has-text-justified">
          <p>
            Furthermore, to evaluate the effect of additional localization
            errors on dynamic object removal performance, we adopt the
            pose estimates from KISS-ICP as input to evaluate. The
            results are presented in Table. IV. When localization errors
            are introduced, the performance of all methods degrades, with
            visibility-based methods being particularly affected. Benefiting
            from the binary occupancy comparison and GMM descriptor
            in the fine dynamic removal stage, our method exhibits a
            certain level of robustness to localization errors and achieves
            satisfactory performance.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table4.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>



        <br>
        <br><br>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Qualitative Results</h2>

        <div class="content has-text-justified">
          <p>
            To intuitively illustrate the performance differences between
            our method and baseline approaches, we present qualitative results on the SemanticKITTI, HeLiMOS, and M2UD
            datasets. Figure presents the accuracy and recall results for
            dynamic object removal on the SemanticKITTI and HeLiMOS
            datasets.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/figure7.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <p>
          To further evaluate the effectiveness of our method for
          dynamic object removal on robots equipped with low-cost
          sensors in real-world scenarios, we conduct experiments on
          the self-collected M2UD [26] dataset. Specifically, we select
          park sequences with low dynamics involving pedestrians and
          cyclists, as well as urban sequences containing various types
          of highly dynamic vehicles. Pose estimation is provided using
          GR-LOAM [34]. The results are shown in Figure, STEP-Mover achieves high-precision dynamic point removal even
          when deployed on a low-speed robot equipped with lowcost sensors. In park scenarios with heavy tree occlusion,
          our method accurately identifies dynamic points. In highly
          dynamic urban environments, the proposed algorithm effectively remove dense dynamic objects. Furthermore, the static
          structures in the scenes are largely preserved, which can be
          attributed to the fine static retrieval module.
        </p>
        <div class="hero-body">
          <img class="rounded" src="./web/resources/figure8.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <br>
        <br><br>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Ablation Study</h2>

        <div class="content has-text-justified">
          <p>
            To further investigate the contribution of each component
            in our algorithm, we conducted an ablation study focusing on
            voxel resolution and individual modules. Figure illustrates the
            effect of varying voxel resolutions in the sy-axis on dynamic
            object rejection performance.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/figure9.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <p>
          Table. V presents the impact of different modules on overall
          system performance.
        </p>
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table5.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <br>
        <br><br>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Runtime Analysis</h2>

        <div class="content has-text-justified">
          <p>
            Table. VI presents the computational time of each module
            in the proposed method and compares it with that of the baselines. The proposed method achieves real-time performance
            across various LiDAR types. Furthermore, the coarse-to-fine-to-refine design effectively reduces the number of voxels processed by subsequent modules using a rough yet efficient strategy, thereby achieving a favorable trade-off between accuracy
            and efficiency. However, the fine retrieval module consumes a
            significant portion of the computational time. Further analysis
            reveals that even with coarsely extracted ground points as
            prior guidance, the ground fitting module still consumes the
            majority of the runtime in the entire pipeline. Compared with
            baselines, the proposed method demonstrates superior and
            stable efficiency across LiDAR types, highlighting its potential
            for integration into SLAM systems.
          </p>
        </div>
        
        <div class="hero-body">
          <img class="rounded" src="./web/resources/table6.png" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </div>

        <br>
        <br><br>
    </div>

  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Additional Results</h2>

        <h3 class="title is-4">Comparison with RobustNeRF</h3>
        <div class="content has-text-justified">
          <p>
            RobustNeRF employs hard thresholding to eliminate distractors, which makes it sensitive to the threshold value and may not generalize effectively in complex scenes.
            Our method is more robust to the distractors and can handle more complicated scenes.

          </p>
        </div>
        
        <div class="content has-text-centered" style="width: 80%; display: flex; justify-content: center; align-items: center">
          <video class="video" width="100%" id="xyalias1" loop playsinline autoplay muted src="web/resources/bellevue_pj.mp4" onplay="resizeAndPlay(this)" style="height: 0px;"></video>
          <canvas height=0 class="videoMerge" id="xyalias1Merge" style="width: 80%;"></canvas>
        </div>
        
        <div class="content has-text-centered" style="width: 80%; display: flex; justify-content: center; align-items: center">
          <video class="video" width="100%" id="xyalias2" loop playsinline autoplay muted src="web/resources/rigi_pj.mp4" onplay="resizeAndPlay(this)" style="height: 0px;"></video>
          <canvas height=0 class="videoMerge" id="xyalias2Merge" style="width: 80%;"></canvas>
        </div>
        

        <h3 class="title is-4">Comparison with NeRF-W</h3>
        <div class="content has-text-justified">
          <p>
            Compare with NeRF-W, our method can handle more complicated scenes with higher occlusion ratio. 
            Furthermore, it does not depend on transient embedding, which adds extra complexity and can potentially result in the loss of high-frequency details.
          </p>
        </div>

        <div class="content has-text-centered" style="width: 80%; display: flex; justify-content: center; align-items: center">
          <video class="video" width="100%" id="xyalias9" loop playsinline autoplay muted src="web/resources/bahnhof_pj.mp4" onplay="resizeAndPlay(this)" style="height: 0px;"></video>
          <canvas height=0 class="videoMerge" id="xyalias9Merge" style="width: 80%;"></canvas>
        </div>
        <div class="content has-text-centered" style="width: 80%; display: flex; justify-content: center; align-items: center">
          <video class="video" width="100%" id="xyalias10" loop playsinline autoplay muted src="web/resources/polybahn_pj.mp4" onplay="resizeAndPlay(this)" style="height: 0px;"></video>
          <canvas height=0 class="videoMerge" id="xyalias10Merge" style="width: 80%;"></canvas>
        </div>

        <div class="content has-text-justified">
          <p>
            Here, we show more comparisons with NeRF-W and RobustNeRF. 
          </p>
        </div>

        <div class="container">
          <ul class="nav nav-tabs nav-fill nav-justified" id="object-scale-recon">
              <li class="nav-item">
                <a class="nav-link active" onclick="objectSceneEvent(0)">station</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(1)">patio-high</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(2)">arc de triomphe</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(3)">drone</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(4)">tree</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(5)">mountain</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(6)">spot</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="objectSceneEvent(7)">corner</a>
              </li>
          </ul>
          <div class="b-dics">
              <img src="web/resources/self/half_bahnhof/nerfw.png" alt="NeRF-W">
              <img src="web/resources/self/half_bahnhof/robust.png" alt="RobustNeRF">
              <img src="web/resources/self/half_bahnhof/ours.png" alt="NeRF On-the-go(ours)">
              <img src="web/resources/self/half_bahnhof/gt.png" alt="GT">
          </div>
        </div>

        <br>
        <br><br>
    </div>

  </div>
</section> -->

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Ren2024NeRF,
    title={NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild},
    author={Ren, Weining and Zhu, Zihan and Sun, Boyang and Chen, Jiaqi and Pollefeys, Marc and Peng, Songyou},
    booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024},
}</code></pre>
  </div>
</section> -->

<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    We thank the Max Planck ETH Center for Learning Systems (CLS) for supporting Songyou Peng. 
We also thank Yiming Zhao and Clément Jambon for helpful discussions.
  </div>
</section> -->

<!-- <section class="section" id="References">
  <div class="container is-max-desktop content">

        <h3 class="title is-4">References</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <a href="https://robustnerf.github.io/" target="_blank">RobustNeRF: Ignoring Distractors with Robust Losses</a>
            </li>
            <li>
              <a href="https://nerf-w.github.io/" target="_blank">NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</a>
            </li>
          </ul>
        </div>
      </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/autonomousvision/mip-splatting">mip-splatting</a>, which is built upon <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template. 
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
